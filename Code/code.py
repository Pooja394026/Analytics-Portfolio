



import importlib
import subprocess
import sys

required_packages = [
    "pandas",
    "numpy",
    "matplotlib",
    "seaborn",
    "geopandas",
    "shap",
    "pgeocode",
    "folium",
    "suntime",
    "scikit-learn",
    "xgboost",
    "lightgbm",
    "statsmodels",
    "requests"
]

def install_if_missing(package):
    try:
        importlib.import_module(package.replace('-', '_'))
    except ImportError:
        subprocess.check_call([sys.executable, "-m", "pip", "install", package])

for pkg in required_packages:
    install_if_missing(pkg)


import os
import io
import gzip
import shutil
import requests
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import geopandas as gpd
import shap
import pgeocode
import folium
import warnings
from io import BytesIO


from suntime import Sun
from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold
from sklearn.metrics import (
    roc_auc_score,
    accuracy_score,
    precision_score,
    recall_score,
    precision_recall_curve,
    average_precision_score
)
from sklearn.preprocessing import StandardScaler, LabelEncoder, label_binarize
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.miscmodels.ordinal_model import OrderedModel


warnings.filterwarnings("ignore")
    

os.getcwd()




# Manually import each filtered CSV
crashes_df = pd.read_csv("Data/Motor_Vehicle_Collisions_-_Crashes_20220629.csv", low_memory=False)
vehicles_df = pd.read_csv("Data/Motor_Vehicle_Collisions_-_Vehicles_20220629.csv", low_memory=False)
persons_df = pd.read_csv("Data/Motor_Vehicle_Collisions_-_Person_20220629.csv", low_memory=False)

# Step 3: Print column names for each dataframe
print("Crashes columns:", crashes_df.columns.tolist())
print("Vehicles columns:", vehicles_df.columns.tolist())
print("Persons columns:", persons_df.columns.tolist())


# Step 4: Filter persons_df for drivers
drivers_df = persons_df[persons_df['PED_ROLE'] == 'Driver'].copy()

# Step 5: Join with vehicles_df â€” only overlapping columns get suffixes
merged_df = drivers_df.merge(
vehicles_df,
left_on='VEHICLE_ID',
right_on='UNIQUE_ID',
how='inner',
suffixes=('_p', '_v')  # will rename only shared columns like CRASH_DATE_p, CRASH_DATE_v
)


print("Merged df columns:", merged_df.columns.tolist())



# Step 7: Flag responsible drivers based on 4 contributing factors
def is_likely_responsible(cf1_p, cf2_p, cf1_v, cf2_v):
    values = [cf1_p, cf2_p, cf1_v, cf2_v]
    clean = [str(x).strip() if pd.notna(x) else 'Unspecified' for x in values]
    return not all(x in ['', 'Unspecified'] for x in clean)

# Apply function
merged_df['likely_responsible'] = merged_df.apply(
lambda row: int(is_likely_responsible(
row['CONTRIBUTING_FACTOR_1_p'],
row['CONTRIBUTING_FACTOR_2_p'],
row['CONTRIBUTING_FACTOR_1_v'],
row['CONTRIBUTING_FACTOR_2_v']
)),
axis=1
)

# Keep only likely responsible drivers
merged_df = merged_df[merged_df['likely_responsible'] == 1].copy()

# Check resulting row count
print("Remaining responsible drivers:", merged_df.shape[0])



# Step 8: Join merged_df (driver+vehicle) with crashes_df using suffixes for overlaps
final_df = merged_df.merge(
crashes_df,
left_on='COLLISION_ID_p',
right_on='COLLISION_ID',
how='left',
suffixes=('', '_c')  # Only overlapping crash columns will get _c
)

print("Final df columns:", final_df.columns.tolist())




# Step 9: Select final columns and compute severity

selected_cols = [
# Driver (person) features
'UNIQUE_ID_p', 'COLLISION_ID_p', 'CRASH_DATE_p', 'CRASH_TIME_p',
'PERSON_AGE', 'CONTRIBUTING_FACTOR_1_p', 'CONTRIBUTING_FACTOR_2_p', 'PERSON_SEX',

# Vehicle features
'STATE_REGISTRATION', 'VEHICLE_TYPE', 'VEHICLE_MAKE', 'VEHICLE_YEAR',
'DRIVER_LICENSE_STATUS', 'DRIVER_LICENSE_JURISDICTION',
'PRE_CRASH', 'POINT_OF_IMPACT',
'CONTRIBUTING_FACTOR_1_v', 'CONTRIBUTING_FACTOR_2_v',

# Crash features
'BOROUGH', 'ZIP CODE', 'LATITUDE', 'LONGITUDE',

# To compute severity
'NUMBER OF PERSONS INJURED', 'NUMBER OF PERSONS KILLED',
'NUMBER OF PEDESTRIANS INJURED', 'NUMBER OF PEDESTRIANS KILLED',
'NUMBER OF CYCLIST INJURED', 'NUMBER OF CYCLIST KILLED',
'NUMBER OF MOTORIST INJURED', 'NUMBER OF MOTORIST KILLED'
]


# Create subset
model_df = final_df[selected_cols].copy()

# Step: Ensure injury-related columns are numeric
injury_cols = [
    'NUMBER OF PERSONS INJURED', 'NUMBER OF PERSONS KILLED',
    'NUMBER OF PEDESTRIANS INJURED', 'NUMBER OF PEDESTRIANS KILLED',
    'NUMBER OF CYCLIST INJURED', 'NUMBER OF CYCLIST KILLED',
    'NUMBER OF MOTORIST INJURED', 'NUMBER OF MOTORIST KILLED'
]

for col in injury_cols:
    model_df[col] = pd.to_numeric(model_df[col], errors='coerce').fillna(0).astype(int)

# Step: Assign ordinal severity levels
def assign_severity(row):
    total_injured = (
        row['NUMBER OF PERSONS INJURED'] +
        row['NUMBER OF PEDESTRIANS INJURED'] +
        row['NUMBER OF CYCLIST INJURED'] +
        row['NUMBER OF MOTORIST INJURED']
    )
    total_killed = (
        row['NUMBER OF PERSONS KILLED'] +
        row['NUMBER OF PEDESTRIANS KILLED'] +
        row['NUMBER OF CYCLIST KILLED'] +
        row['NUMBER OF MOTORIST KILLED']
    )
    
    n=2
    if total_killed > 0 or total_injured > n:
        return 'Fatal'
    elif total_injured> 0 and total_injured<=n:
        return 'Severe'
    else:
        return 'Non Severe'

# Step: Apply and convert to object dtype
model_df['Severity'] = model_df.apply(assign_severity, axis=1).astype('object')

model_df['Severity'].value_counts()



# Drop the raw injury/fatality columns (optional)
model_df.drop(columns=[
'NUMBER OF PERSONS INJURED', 'NUMBER OF PERSONS KILLED',
'NUMBER OF PEDESTRIANS INJURED', 'NUMBER OF PEDESTRIANS KILLED',
'NUMBER OF CYCLIST INJURED', 'NUMBER OF CYCLIST KILLED',
'NUMBER OF MOTORIST INJURED', 'NUMBER OF MOTORIST KILLED'
], inplace=True)

# Check results
print("Final modeling dataframe columns:", model_df.columns.tolist())
print("Target distribution:\n", model_df['Severity'].value_counts())



# Step 10: Combine CRASH_DATE_p and CRASH_TIME_p into a single datetime column
model_df['CRASH_DATETIME'] = pd.to_datetime(
model_df['CRASH_DATE_p'].astype(str) + ' ' + model_df['CRASH_TIME_p'].astype(str),
errors='coerce'
)

# Drop the original separate columns
model_df.drop(columns=['CRASH_DATE_p', 'CRASH_TIME_p'], inplace=True)



#Info save
# Capture .info() output
buffer = io.StringIO()
model_df.drop(['CONTRIBUTING_FACTOR_1_p', 'CONTRIBUTING_FACTOR_2_p'], axis=1).info(buf=buffer)
info_str = buffer.getvalue()

# Plot and save as image
plt.figure(figsize=(12, 6))
plt.text(0, 1, info_str, fontsize=12, family='monospace', va='top')
plt.axis('off')
plt.tight_layout()
plt.savefig("Figures/model_df_info_filtered.png", dpi=300, bbox_inches='tight')
plt.close()







# Define station ID and years
station_id = "KJRB0"  # Central Park, NYC
years = [2022, 2023, 2024, 2025]
base_url = "https://data.meteostat.net/hourly"

# Columns to keep and rename mapping
weather_columns = {
    "temp": "temperature_celsius",
    "rhum": "humidity_percent",
    "prcp": "precipitation_mm",
    "wspd": "wind_speed_kph",
    "coco": "weather_code"
}

# Mapping for 'coco' values
coco_map = {
    1: "Clear", 2: "Fair", 3: "Cloudy", 4: "Overcast", 5: "Fog",
    6: "Freezing Fog", 7: "Light Rain", 8: "Rain", 9: "Heavy Rain",
    10: "Freezing Rain", 11: "Heavy Freezing Rain", 12: "Sleet",
    13: "Heavy Sleet", 14: "Light Snow", 15: "Snow", 16: "Heavy Snow",
    17: "Snow Grains", 18: "Ice Crystals", 19: "Ice Pellets",
    20: "Hail", 21: "Thunderstorm", 22: "Thunderstorm with Hail"
}

# Download and combine
all_weather_data = []

for year in years:
    url = f"{base_url}/{year}/{station_id}.csv.gz"
    print(f"Downloading {url}...")
    
    try:
        response = requests.get(url)
        response.raise_for_status()

        with gzip.open(BytesIO(response.content), 'rt') as f:
            df = pd.read_csv(f)

        # Combine datetime
        df["datetime"] = pd.to_datetime(df[['year', 'month', 'day', 'hour']])
        df = df.set_index("datetime")
        df = df.rename(columns=weather_columns)
        df = df[list(weather_columns.values())]  # Keep only renamed columns

        # Map weather codes
        df["weather_desc"] = df["weather_code"].map(coco_map)
        
        # Optional: drop original weather_code if not needed
        df.drop(columns=["weather_code"], inplace=True)

        all_weather_data.append(df)

    except Exception as e:
        print(f"Failed to download or process {url}: {e}")

# Combine all years
weather_df = pd.concat(all_weather_data)
print("Combined weather shape:", weather_df.shape)
print(weather_df.head())







# Step 11: Round CRASH_DATETIME to the nearest hour
model_df['CRASH_DATETIME_1H'] = model_df['CRASH_DATETIME'].dt.floor('1H')

# If weather_df is still indexed by datetime, reset it
weather_df = weather_df.reset_index()

# Step 12: Merge weather data on datetime
model_df = model_df.merge(
    weather_df,
    left_on='CRASH_DATETIME_1H',
    right_on='datetime',
    how='left'
)


# Drop extra datetime column from weather data
model_df.drop(columns=['CRASH_DATETIME_1H'], inplace=True)





# Step 13: Set NYC coordinates and create Sun object
LAT, LON = 40.7128, -74.0060
sun = Sun(LAT, LON)


# Step 15: Extract crash date only (for sunrise/sunset lookup)
model_df['crash_date'] = model_df['CRASH_DATETIME'].dt.date

# Step 16: Build lookup of sunrise/sunset times for each date
unique_dates = model_df['crash_date'].unique()
sun_times = {
    date: {
        'sunrise': sun.get_local_sunrise_time(date),
        'sunset': sun.get_local_sunset_time(date)
    }
    for date in unique_dates
}

# Step 17: Map sunrise and sunset back to the dataframe
model_df['sunrise'] = model_df['crash_date'].map(lambda d: sun_times[d]['sunrise'])
model_df['sunset'] = model_df['crash_date'].map(lambda d: sun_times[d]['sunset'])

# Step 18: Flag whether crash happened during sunlight or not
def get_sunlight_label(crash_time, sunrise, sunset):
    if pd.isna(crash_time) or pd.isna(sunrise) or pd.isna(sunset):
        return None
    return 'Sunlight' if sunrise <= crash_time < sunset else 'No Sunlight'

model_df['sunlight_presence'] = model_df.apply(
    lambda row: get_sunlight_label(row['CRASH_DATETIME'], row['sunrise'], row['sunset']),
    axis=1
)

# Step 19: Drop helper columns used for mapping
model_df.drop(columns=['crash_date', 'sunrise', 'sunset'], inplace=True)

# (Optional) Check distribution
print(model_df['sunlight_presence'].value_counts())


# Step 20: Categorize time of day
def get_time_of_day(dt):
    if pd.isna(dt):
        return None
    hour = dt.hour
    if 0 <= hour < 5:
        return 'Late Night'
    elif 5 <= hour < 8:
        return 'Early Morning'
    elif 8 <= hour < 12:
        return 'Morning'
    elif 12 <= hour < 17:
        return 'Afternoon'
    elif 17 <= hour < 21:
        return 'Evening'
    else:
        return 'Night'
    


model_df['time_of_day'] = model_df['CRASH_DATETIME'].apply(get_time_of_day)

# (Optional) View distribution
print(model_df['time_of_day'].value_counts())


# Step 21: Weekday vs Weekend
model_df['weekday_or_weekend'] = model_df['CRASH_DATETIME'].dt.weekday.apply(
    lambda x: 'Weekend' if x >= 5 else 'Weekday'
)


# Step 22: Assign Season (Northern Hemisphere)
def get_season(month):
    if month in [12, 1, 2]:
        return 'Winter'
    elif month in [3, 4, 5]:
        return 'Spring'
    elif month in [6, 7, 8]:
        return 'Summer'
    elif month in [9, 10, 11]:
        return 'Fall'
    else:
        return None



model_df['season'] = model_df['CRASH_DATETIME'].dt.month.apply(get_season)


# Step 23: Flag vehicles and person registered in NY vs out-of-state
model_df['in_state_veh'] = model_df['STATE_REGISTRATION'].apply(
    lambda x: 1 if str(x).strip().upper() == 'NY' else 0
)

model_df['in_state_lic'] = model_df['DRIVER_LICENSE_JURISDICTION'].apply(
    lambda x: 1 if str(x).strip().upper() == 'NY' else 0
)


# Drop extra columns 
model_df.drop(columns=['STATE_REGISTRATION','DRIVER_LICENSE_JURISDICTION'], inplace=True)








# Step 24: Consolidate vehicle contributing factors
model_df['CONTRIBUTING_FACTOR_v'] = model_df['CONTRIBUTING_FACTOR_1_v']

# Replace if value is missing or 'Unspecified'
mask = model_df['CONTRIBUTING_FACTOR_v'].isna() | (model_df['CONTRIBUTING_FACTOR_v'].str.strip().str.lower() == 'unspecified')
model_df.loc[mask, 'CONTRIBUTING_FACTOR_v'] = model_df.loc[mask, 'CONTRIBUTING_FACTOR_2_v']

# Drop the original contributing factor columns
model_df.drop(columns=['CONTRIBUTING_FACTOR_1_v', 'CONTRIBUTING_FACTOR_2_v'], inplace=True)















# Step 25: Map CONTRIBUTING_FACTOR_v to UBI behavior group


# Define the updated mapping dictionary
factor_to_ubi_group = {
    # driver_distraction
    "Driver Inattention/Distraction": "Driver Distraction",
    "Listening/Using Headphones": "Driver Distraction",

    # under_influence
    "Alcohol Involvement": "Under Influence",
    "Drugs (illegal)": "Under Influence",

    # driver_health_issue
    "Fell Asleep": "Driver Health Issue",
    "Fatigued/Drowsy": "Driver Health Issue",
    "Lost Consciousness": "Driver Health Issue",
    "Illnes": "Driver Health Issue",

    # rule_violation
    "Failure to Yield Right-of-Way": "Rule Violation",
    "Driver Inexperience": "Rule Violation",
    "Failure to Keep Right": "Rule Violation",
    "Traffic Control Disregarded": "Rule Violation",
    "Unsafe Speed": "Rule Violation",

    # unsafe_maneuver
    "Aggressive Driving/Road Rage": "Unsafe Maneuver",
    "Unsafe Lane Changing": "Unsafe Maneuver",
    "Turning Improperly": "Unsafe Maneuver",
    "Passing or Lane Usage Improper": "Unsafe Maneuver",
    "Backing Unsafely": "Unsafe Maneuver",
    "Following Too Closely": "Unsafe Maneuver",
    "Passing Too Closely": "Unsafe Maneuver",

    # environment_risk
    "Pavement Slippery": "Environment Risk",
    "Obstruction/Debris": "Environment Risk",
    "Pavement Defective": "Environment Risk",
    "Animals Action": "Environment Risk",
    "Outside Car Distraction": "Environment Risk",
    "Traffic Control Device Improper/Non-Working": "Environment Risk",
    "Lane Marking Improper/Inadequate": "Environment Risk",
    "Glare": "Environment Risk",
    "Shoulders Defective/Improper": "Environment Risk",
    "View Obstructed/Limited": "Environment Risk",
    "Pedestrian/Bicyclist/Other Pedestrian Error/Confusion": "Environment Risk",
    "Reaction to Uninvolved Vehicle": "Environment Risk",

    # vehicle_defect
    "Brakes Defective": "Vehicle Defect",
    "Steering Failure": "Vehicle Defect",
    "Tow Hitch Defective": "Vehicle Defect",
    "Accelerator Defective": "Vehicle Defect",
    "Headlights Defective": "Vehicle Defect",
    "Tire Failure/Inadequate": "Vehicle Defect",
    "Windshield Inadequate": "Vehicle Defect"
}

# Apply the mapping
model_df["ubi_behavior_group"] = model_df["CONTRIBUTING_FACTOR_v"].map(factor_to_ubi_group).fillna("other_factors")



model_df = model_df[model_df["CONTRIBUTING_FACTOR_v"] != "Other Vehicular"]


# Step 27: Map VEHICLE_TYPE to simplified categories

vehicle_type_mapping = {
    # Small Passenger Vehicle
    "Sedan": "Small Passenger Vehicle",

    # Large Passenger Vehicle
    "Station Wagon/Sport Utility Vehicle": "Large Passenger Vehicle",
    "Van": "Large Passenger Vehicle",

    # Two-Wheel & Micro-Mobility
    "Bike": "Two-Wheel & Micro-Mobility",
    "E-Bike": "Two-Wheel & Micro-Mobility",
    "E-Scooter": "Two-Wheel & Micro-Mobility",
    "Moped": "Two-Wheel & Micro-Mobility",
    "Motorcycle": "Two-Wheel & Micro-Mobility",

    # Light Truck/Utility
    "Pick-up Truck": "Light Truck/Utility"
}

# Apply the mapping and handle unknown types
model_df["VEHICLE_TYPE_GROUP"] = model_df["VEHICLE_TYPE"].map(vehicle_type_mapping).fillna("Others")


# Drop extra columns 
model_df.drop(columns=['VEHICLE_MAKE','VEHICLE_TYPE'], inplace=True)


# Step 28: Fill missing values in DRIVER_LICENSE_STATUS
model_df["DRIVER_LICENSE_STATUS"] = model_df["DRIVER_LICENSE_STATUS"].fillna("Missing")



# Step 29: Fill missing values in BOROUGH
model_df["BOROUGH"] = model_df["BOROUGH"].fillna("Missing")



# Step 30: Final cleanup
model_df = model_df.set_index("UNIQUE_ID_p")






# Step 1: Get ZIP centroids using pgeocode
nomi = pgeocode.Nominatim('us')

# Ensure ZIP CODE column is string
model_df['ZIP CODE'] = model_df['ZIP CODE'].astype(str)

# Get unique ZIP codes from your data
zip_list = model_df['ZIP CODE'].unique().tolist()
zip_info = nomi.query_postal_code(zip_list)

# Build ZIP code to lat/lon DataFrame
zip_df = zip_info[['postal_code', 'latitude', 'longitude']].rename(
    columns={'postal_code': 'ZIP CODE', 'latitude': 'LAT', 'longitude': 'LON'}
)

# Step 2: Merge ZIP centroids into your model data
merged = model_df.merge(zip_df, on='ZIP CODE', how='left')

# Step 3: Group by ZIP to calculate count and avg severity
grouped = merged.groupby(['ZIP CODE', 'LAT', 'LON']).agg(
    accident_count=('Severity', 'count'),
    avg_severity=('Severity', 'mean')
).reset_index()

# Step 4: Normalize severity for color scaling
grouped['severity_norm'] = (
    grouped['avg_severity'] - grouped['avg_severity'].min()
) / (grouped['avg_severity'].max() - grouped['avg_severity'].min())

# Step 5: Create folium map centered on NYC
m = folium.Map(location=[40.7128, -74.0060], zoom_start=11)

# Step 6: Add ZIP-level circle markers
for _, row in grouped.dropna(subset=['LAT', 'LON']).iterrows():
    folium.CircleMarker(
        location=(row['LAT'], row['LON']),
        radius=row['accident_count'] ** 0.5,  # Square root for scaling
        fill=True,
        fill_color=f'rgba(255,0,0,{row["severity_norm"]:.2f})',
        fill_opacity=row['severity_norm'],
        stroke=False,
        popup=f"ZIP: {row['ZIP CODE']}<br>Count: {row['accident_count']}<br>Avg Severity: {row['avg_severity']:.2f}"
    ).add_to(m)

# Step 7: Save map
m.save("Figures/accident_map_by_zip.html")






# Outlier
# Overwrite model_df with filtered data
model_df = model_df[
    (model_df['PERSON_AGE'].between(16, 114, inclusive='both')) &
    (model_df['VEHICLE_YEAR'] <= 2025)
]
















# Capture info output
buffer = io.StringIO()
model_df.drop(
["CONTRIBUTING_FACTOR_1_p", "CONTRIBUTING_FACTOR_2_p", "PRE_CRASH", "POINT_OF_IMPACT", "datetime"],
axis=1
).info(buf=buffer)

info_str = buffer.getvalue()

# Plot as image
plt.figure(figsize=(12, 6))
plt.text(0, 1, info_str, fontsize=12, family='monospace', va='top')
plt.axis('off')
plt.tight_layout()
plt.savefig("Figures/model_df_info.png", dpi=300, bbox_inches='tight')
plt.close()


# Set seaborn style
sns.set(style="whitegrid")
plt.rcParams["figure.figsize"] = (12, 6)












# Drop specified columns
columns_to_drop = [
    "COLLISION_ID_p", 
    "CONTRIBUTING_FACTOR_1_p", 
    "CONTRIBUTING_FACTOR_2_p", 
    "PRE_CRASH", 
    "POINT_OF_IMPACT", 
    "ZIP CODE", 
    "LATITUDE", 
    "LONGITUDE", 
    "CRASH_DATETIME", 
    "datetime",
    "CONTRIBUTING_FACTOR_v"
]

model_df = model_df.drop(columns=columns_to_drop)


# Step 31: Fill missing weather descriptions
model_df["weather_desc"] = model_df["weather_desc"].fillna("Missing")





# model_df.to_csv('tempp.csv')



# Print unique values for each categorical column
categorical_cols = model_df.select_dtypes(include='object').columns

for col in categorical_cols:
    print(f"\n--- {col} ---")
    print(model_df[col].unique())




# Step 31.5: Fill missing values in PERSON_SEX with 'U'
model_df['PERSON_SEX'] = model_df['PERSON_SEX'].fillna('U')



model_df.rename(columns={
    'PERSON_AGE': 'Age',
    'PERSON_SEX': 'Sex',
    'VEHICLE_YEAR': 'Vehicle Year',
    'DRIVER_LICENSE_STATUS': 'License Status',
    'BOROUGH': 'Borough',
    'Severity': 'Severity',
    'temperature_celsius': 'Temperature C',
    'humidity_percent': 'Humidity Percent',
    'precipitation_mm': 'Precipitation MM',
    'wind_speed_kph': 'Wind Speed KPH',
    'weather_desc': 'Weather',
    'sunlight_presence': 'Sunlight Presence',
    'time_of_day': 'Time of Day',
    'weekday_or_weekend': 'Weekday or Weekend',
    'season': 'Season',
    'in_state_veh': 'In State Vehicle',
    'in_state_lic': 'In State License',
    'ubi_behavior_group': 'Driving Behavior Group',
    'VEHICLE_TYPE_GROUP': 'Vehicle Type Group'
}, inplace=True)





# Step 1: Save model_df.info() output as an image
buffer = io.StringIO()
model_df.info(buf=buffer)
info_str = buffer.getvalue()

plt.figure(figsize=(12, 6))
plt.text(0, 1, info_str, fontsize=12, family='monospace', va='top')
plt.axis('off')
plt.tight_layout()
plt.savefig("Figures/model_df_info.png", dpi=300, bbox_inches='tight')
plt.close()

# Step 2: Set plotting style
sns.set(style="whitegrid")
plt.rcParams["figure.figsize"] = (12, 6)

# Step 3: Define actual features from your DataFrame
categorical_features = [
    'Sex', 'License Status', 'Borough', 'Weather', 'Sunlight Presence',
    'Time of Day', 'Weekday or Weekend', 'Season',
    'Driving Behavior Group', 'Vehicle Type Group'
]

continuous_features = [
    'Age', 'Vehicle Year', 'Temperature C', 'Humidity Percent',
    'Precipitation MM', 'Wind Speed KPH'
]

# Step 4: Severity color palette
severity_palette = {
    'Non Severe': '#66c2a5',   # soft green
    'Severe': '#ffd92f',  # warm gold
    'Fatal': '#fc8d62'  # coral red
}

# Step 5: Categorical feature plots (original + high-only)
for feature in categorical_features:
    prop_df = (
        model_df
        .groupby([feature, 'Severity'])
        .size()
        .unstack(fill_value=0)
        .apply(lambda x: x / x.sum(), axis=1)
        .reset_index()
        .melt(id_vars=feature, var_name='Severity', value_name='Proportion')
    )

    # --- Full bar plot ---
    max_y = prop_df['Proportion'].max()

    plt.figure(figsize=(14, 6))
    sns.barplot(
        data=prop_df,
        x=feature,
        y='Proportion',
        hue='Severity',
        palette=severity_palette
    )
    plt.ylim(0, max_y + 0.05)
    #plt.title(f'Severity Distribution by {feature}', fontsize=14)
    plt.xticks(rotation=45, ha='right')
    plt.ylabel('Proportion')
    plt.xlabel(feature)
    plt.legend(title='Severity')
    plt.tight_layout()
    plt.savefig(f'Figures/Severity_by_{feature}.png')
    plt.close()

    # --- Fatal only plot (scaled to high only) ---
    prop_df_high = prop_df[prop_df['Severity'] == 'Fatal']
    max_high = prop_df_high['Proportion'].max()

    plt.figure(figsize=(14, 6))
    sns.barplot(
        data=prop_df_high,
        x=feature,
        y='Proportion',
        hue='Severity',
        palette={'Fatal': severity_palette['Fatal']}
    )
    plt.ylim(0, max_high + 0.05)
    #plt.title(f'Fatal Proportion by {feature}', fontsize=14)
    plt.xticks(rotation=45, ha='right')
    plt.ylabel('Proportion')
    plt.xlabel(feature)
    plt.legend(title='Severity')
    plt.tight_layout()
    plt.savefig(f'Figures/Severity_by_{feature}_high_only.png')
    plt.close()

# Step 6: Continuous feature plots (boxplots)
for feature in continuous_features:
    plt.figure(figsize=(10, 6))
    sns.boxplot(
        data=model_df,
        x='Severity',
        y=feature,
        palette=severity_palette
    )
    #plt.title(f'{feature} by Severity', fontsize=14)
    plt.xlabel('Severity')
    plt.ylabel(feature)
    plt.tight_layout()
    plt.savefig(f'Figures/p_{feature}_by_Severity.png')
    plt.close()





# Heat map - ZIP Code Level (Simplified Debug Version)


print("Starting ZIP code heat map creation...")

# Step 1: Create the ZIP code mapping from final_df
print("Step 1: Creating ZIP code mapping...")
df_map_zip = final_df[['UNIQUE_ID_p', 'ZIP CODE']].copy()
print(f"ZIP code mapping shape: {df_map_zip.shape}")
print(f"Sample ZIP codes from data: {df_map_zip['ZIP CODE'].head().tolist()}")

# Step 2: Assign injury cost values
print("Step 2: Assigning injury costs...")
severity_cost = {"Non Severe": 5700, "Severe": 80000, "Fatal": 1800000}
model_df["InjuryCost"] = model_df["Severity"].map(severity_cost)

# Step 3: Merge model_df with ZIP code mapping
print("Step 3: Merging with model data...")
model_df_with_zip = model_df.merge(df_map_zip, left_index=True, right_on='UNIQUE_ID_p', how='left')
print(f"Model data with ZIP shape: {model_df_with_zip.shape}")

# Step 4: Filter out missing ZIP codes
print("Step 4: Filtering missing ZIP codes...")
df_filtered = model_df_with_zip.dropna(subset=['ZIP CODE'])
df_filtered = df_filtered[df_filtered['ZIP CODE'] != 'Missing']
df_filtered = df_filtered[df_filtered['ZIP CODE'] != '']
df_filtered = df_filtered[df_filtered['ZIP CODE'].astype(str) != 'nan']
print(f"Filtered data shape: {df_filtered.shape}")
print(f"Unique ZIP codes in filtered data: {df_filtered['ZIP CODE'].nunique()}")
print(f"Sample ZIP codes: {sorted(df_filtered['ZIP CODE'].unique())[:10]}")

# Step 5: Summarize cost by ZIP code
print("Step 5: Summarizing costs by ZIP code...")
zipcode_summary = (
    df_filtered.groupby('ZIP CODE')
    .agg(
        TotalCost=("InjuryCost", "sum"),
        TotalRecords=("InjuryCost", "count")
    )
    .reset_index()
)
zipcode_summary["NormalizedCost"] = zipcode_summary["TotalCost"] / zipcode_summary["TotalRecords"]
print(f"ZIP code summary shape: {zipcode_summary.shape}")
print(f"Cost summary stats:\n{zipcode_summary['NormalizedCost'].describe()}")

# Step 6: Try to load shapefile
print("Step 6: Loading NYC ZIP code boundaries...")
try:
    # Try GitHub source
    zip_url = "https://raw.githubusercontent.com/fedhere/PUI2015_EC/master/mam1612_EC/nyc-zip-code-tabulation-areas-polygons.geojson"
    nyc_zipcodes = gpd.read_file(zip_url)
    print(f"Loaded {len(nyc_zipcodes)} ZIP code boundaries")
    print(f"Available columns: {list(nyc_zipcodes.columns)}")
    
    # Check for ZIP code column
    zip_col = None
    for col in ['postalCode', 'zipcode', 'ZIPCODE', 'zip', 'ZIP']:
        if col in nyc_zipcodes.columns:
            zip_col = col
            break
    
    if zip_col:
        nyc_zipcodes['ZIP_CODE'] = nyc_zipcodes[zip_col].astype(str)
        print(f"Using column '{zip_col}' for ZIP codes")
        print(f"Sample shapefile ZIP codes: {nyc_zipcodes['ZIP_CODE'].head().tolist()}")
    else:
        print("No ZIP code column found in shapefile!")
        print("Available columns:", list(nyc_zipcodes.columns))

except Exception as e:
    print(f"Error loading shapefile: {e}")
    # Create a fallback - just plot the summary data as points
    print("Creating fallback visualization...")
    
    # For now, let's just print the summary and exit
    print("ZIP code summary:")
    print(zipcode_summary.head(10))
    
    # Simple bar plot as fallback
    plt.figure(figsize=(12, 8))
    top_zips = zipcode_summary.nlargest(20, 'NormalizedCost')
    plt.barh(range(len(top_zips)), top_zips['NormalizedCost'])
    plt.yticks(range(len(top_zips)), top_zips['ZIP CODE'])
    plt.xlabel('Normalized Cost per Accident ($)')
    plt.ylabel('ZIP Code')
    #plt.title('Top 20 ZIP Codes by Normalized Cost per Accident')
    plt.tight_layout()
    plt.savefig("Figures/zipcode_cost_fallback.png", dpi=300, bbox_inches='tight')

    
    # Clean up and exit
    model_df.drop('InjuryCost', axis=1, inplace=True)
    exit()

# Step 7: Check if we have matching ZIP codes
if len(nyc_zipcodes) == 0:
    print("No shapefile data available")
    exit()

# Clean ZIP codes to remove decimal points
print("Step 7: Cleaning ZIP code formats...")
zipcode_summary['ZIP CODE'] = zipcode_summary['ZIP CODE'].astype(str).str.replace('.0', '', regex=False).str.zfill(5)
nyc_zipcodes['ZIP_CODE'] = nyc_zipcodes['ZIP_CODE'].astype(str).str.zfill(5)

print("After cleaning:")
print("Sample ZIP codes from data:", zipcode_summary['ZIP CODE'].head(10).tolist())
print("Sample ZIP codes from shapefile:", nyc_zipcodes['ZIP_CODE'].head(10).tolist())

# Find matching ZIP codes
common_zips = set(zipcode_summary['ZIP CODE']) & set(nyc_zipcodes['ZIP_CODE'])
print(f"Number of matching ZIP codes: {len(common_zips)}")

if len(common_zips) == 0:
    print("No matching ZIP codes found after cleaning!")
    # Print some samples for debugging
    print("Data ZIP codes sample:", sorted(zipcode_summary['ZIP CODE'].unique())[:10])
    print("Shapefile ZIP codes sample:", sorted(nyc_zipcodes['ZIP_CODE'].unique())[:10])
    exit()

# Step 8: Merge data with shapefile
print("Step 8: Merging data with shapefile...")
nyc_map = nyc_zipcodes.merge(zipcode_summary, left_on='ZIP_CODE', right_on='ZIP CODE', how='inner')
print(f"Merged data shape: {nyc_map.shape}")

# Step 9: Create the map
print("Step 9: Creating the heat map...")

# Ensure proper coordinate system
if nyc_map.crs != 'EPSG:4326':
    nyc_map = nyc_map.to_crs('EPSG:4326')

# Load borough boundaries for labels
try:
    borough_url = "https://raw.githubusercontent.com/dwillis/nyc-maps/master/boroughs.geojson"
    boroughs = gpd.read_file(borough_url)
    boroughs = boroughs.to_crs(nyc_map.crs)
    print("Borough boundaries loaded for labeling")
except Exception as e:
    print(f"Could not load borough boundaries: {e}")
    boroughs = None

# Plot the heat map
fig, ax = plt.subplots(1, 1, figsize=(14, 12))
nyc_map.plot(
    column="NormalizedCost",
    cmap="YlOrRd",
    linewidth=0.5,
    ax=ax,
    edgecolor='0.8',
    legend=True,
    legend_kwds={'shrink': 0.8, 'aspect': 20}
)

# Add borough boundaries and labels
if boroughs is not None:
    boroughs.boundary.plot(ax=ax, linewidth=2, edgecolor='black', alpha=0.7)
    
    # Add borough labels at centroids
    for idx, row in boroughs.iterrows():
        centroid = row['geometry'].centroid
        borough_name = row['BoroName'].upper()
        ax.text(
            centroid.x,
            centroid.y,
            borough_name,
            horizontalalignment='center',
            verticalalignment='center',
            fontsize=12,
            fontweight='bold',
            color='black',
            bbox=dict(boxstyle="round,pad=0.3", facecolor='white', alpha=0.8, edgecolor='black')
        )

# Set map bounds with slight zoom (crop 5% from each edge instead of 10%)
bounds = nyc_map.total_bounds
x_margin = (bounds[2] - bounds[0]) * 0.05
y_margin = (bounds[3] - bounds[1]) * 0.05
ax.set_xlim(bounds[0] + x_margin, bounds[2] - x_margin)
ax.set_ylim(bounds[1] + y_margin, bounds[3] - y_margin)

ax.set_aspect('equal')
#ax.set_title("Estimated Cost per Accident by ZIP Code ($)", fontsize=16, pad=20)
ax.axis("off")
plt.tight_layout()
plt.savefig("Figures/zipcode_normalized_injury_cost_map.png", dpi=300, bbox_inches='tight')
#plt.show()
plt.close()

# Clean up
model_df.drop('InjuryCost', axis=1, inplace=True)




#freq
# Step 1: Define color palette
severity_palette = {
    'Non Severe': '#66c2a5',   # soft green
    'Severe': '#ffd92f',       # warm gold
    'Fatal': '#fc8d62'         # coral red
}



# Step 4: Plot
plt.figure(figsize=(8, 5))
sns.countplot(
    data=model_df,
    x="Severity",
    order=["Non Severe", "Severe", "Fatal"],
    palette=severity_palette
)
#plt.title("Crash Severity Frequency", fontsize=14)
plt.xlabel("Severity")
plt.ylabel("Number of Records")
plt.tight_layout()

# Step 5: Save the plot
plt.savefig("Figures/severity_frequency_plot.png", dpi=300)
plt.close()
print("Saved as 'severity_frequency_plot.png'")








# Step 32: One-hot encode categorical variables, dropping preferred baseline categories
# Each feature may have more than one baseline to drop
# List of columns to one-hot encode
categorical_columns = [
    'Sex',
    'License Status',
    'Borough',
    'Weather',
    'Sunlight Presence',
    'Time of Day',
    'Weekday or Weekend',
    'Season',
    'Driving Behavior Group',
    'Vehicle Type Group'
]

# List of specific dummy columns to drop (baseline categories)
baseline_dummies_to_drop = [
    'Sex_F',
    'Sex_U',
    'License Status_Missing',
    'Borough_Missing',
    'Weather_Missing',
    'Weather_Clear',
    'Sunlight Presence_No Sunlight',
    'Time of Day_Late Night',
    'Weekday or Weekend_Weekday',
    'Season_Summer',
    'Driving Behavior Group_other_factors',
    'Driving Behavior Group_Driver Distraction',
    'Vehicle Type Group_Others'
]





# Drop missing values if needed
model_df.dropna(inplace=True)

# Create dummy variables
model_df_encoded = pd.get_dummies(
    model_df,
    columns=categorical_columns,
    drop_first=False
)

# Drop selected baseline dummies
for dummy_col in baseline_dummies_to_drop:
    if dummy_col in model_df_encoded.columns:
        model_df_encoded.drop(columns=dummy_col, inplace=True)
    else:
        print(dummy_col)






    
model_df_encoded.columns = (
    model_df_encoded.columns
    .str.replace('_', ' - ', regex=False)
    .str.replace('/', ' or ', regex=False)
    .str.replace('&', 'and', regex=False)
)









# --- STEP 1: Preprocessing ---
X = model_df_encoded.drop(columns=['Severity']).apply(pd.to_numeric)
scaler = StandardScaler()
X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)

# --- STEP 2: VIF Function ---
def compute_vif(df):
    vif = pd.DataFrame()
    vif['Feature'] = df.columns
    vif['VIF'] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]
    return vif.sort_values(by='VIF', ascending=False).reset_index(drop=True)

# --- STEP 3: Iterative Removal ---
iteration = 1
while True:
    vif_result = compute_vif(X_scaled)
    top_vif = vif_result.iloc[0]
    
    if top_vif['VIF'] < 10:
        print(f"\n All VIFs are below 10. Final list generated.")
        break

    print(f"\n Iteration {iteration}")
    print(f"Removing: {top_vif['Feature']} (VIF={top_vif['VIF']:.2f})")
    
    X_scaled = X_scaled.drop(columns=top_vif['Feature'])
    iteration += 1

# --- STEP 4: Save Top 10 VIF Table ---
vif_result['VIF']=vif_result['VIF'].round(2)
top_10_vif = vif_result[['Feature', 'VIF']].head(10)

fig, ax = plt.subplots(figsize=(8, 0.5 * len(top_10_vif)))
ax.axis('off')
table = ax.table(cellText=top_10_vif.values,
                 colLabels=top_10_vif.columns,
                 cellLoc='center',
                 loc='center')
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1, 1.5)

plt.savefig("Figures/top_10_vif_report.png", bbox_inches='tight', dpi=300)
plt.close()

print("Saved top 10 VIF report as 'top_10_vif_report.png'")








    
# Step 37: Define features and target
X = model_df_encoded.drop(columns=['Severity'])
y = model_df_encoded['Severity']




# Step 0: Ordinal encoding
severity_order = {'Non Severe': 0, 'Severe': 1, 'Fatal': 2}
y_ordinal = y.map(severity_order)

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X, y_ordinal)

# Step 2: Get and sort feature importances
importances = pd.Series(rf_model.feature_importances_, index=X.columns)
importances = importances.sort_values(ascending=False)

# Step 1: Select top 45 features
selected_features = list(importances.index[:45])
X_selected = X[selected_features]

# Step 2: Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_selected)
X_scaled_df = pd.DataFrame(X_scaled, columns=selected_features)

# Step 3: Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled_df, y_ordinal, test_size=0.1, random_state=42, stratify=y_ordinal
)

# Align indices
X_train = X_train.reset_index(drop=True)
y_train = y_train.reset_index(drop=True)
X_test = X_test.reset_index(drop=True)
y_test = y_test.reset_index(drop=True)

# Step 4: Fit ordinal logistic regression model
model = OrderedModel(
    y_train,
    X_train,
    distr='logit'  # ordinal logit
)
res = model.fit(method='bfgs', disp=False)

# Step 5: Predict and AUC
y_probs = res.model.predict(res.params, exog=X_test)
y_test_array = pd.get_dummies(y_test)
auc_score = roc_auc_score(y_test_array, y_probs, multi_class='ovr')
print("AUC (Ordinal Logistic Regression):", round(auc_score, 3))

# Step 6: Build summary DataFrame manually
summary_df = pd.DataFrame({
    "Variable": res.model.exog_names,
    "Coef.": res.params,
    "Std.Err.": res.bse,
    "P-value": res.pvalues
})

# Step 7: Filter out threshold variables and get top 10
# Filter out variables containing "/" (threshold variables like "1/2", "0/1")
filtered_summary = summary_df[~summary_df["Variable"].str.contains("/", na=False)]

# Calculate absolute coefficient for ranking
filtered_summary["abs_coef"] = filtered_summary["Coef."].abs()

# Get top 10 by magnitude
top10 = filtered_summary.sort_values(by="abs_coef", ascending=False).head(10)
# Make coefficients absolute for display
top10_display = top10.copy()
top10_display["Coef."] = top10_display["Coef."].abs()
top10_table = top10_display[["Variable", "Coef.", "P-value"]].round(3)




# Step 8: Save top 10 table as image
fig, ax = plt.subplots(figsize=(14, 8))
ax.axis('tight')
ax.axis('off')
table = ax.table(cellText=top10_table.values,
                colLabels=top10_table.columns,
                cellLoc='center',
                loc='center')
table.auto_set_font_size(False)
table.set_fontsize(14)
table.scale(1, 2.5)
plt.tight_layout()
plt.savefig("Figures/ordinal_logit_top10_coefficients.png", dpi=300, bbox_inches='tight', pad_inches=0.2)
plt.close()


# Round numeric columns to 3 decimals
rounded_table = top10_table.copy()
for col in rounded_table.select_dtypes(include='number').columns:
    rounded_table[col] = rounded_table[col].round(3)

# Convert to LaTeX
latex_table = rounded_table.to_latex(
    index=False,
    column_format='lcc',
    label='tab:top10_vars',
    caption='Top 10 Variables from Logit Model'
)

# Replace \begin{table} with \begin{table}[H]
latex_table = latex_table.replace(
    "\\begin{table}",
    "\\begin{table}[H]"
)

# Optional: Add newline after caption for formatting
latex_table = latex_table.replace(
    "\\caption{Top 10 Variables from Logit Model}",
    "\\caption{Top 10 Variables from Logit Model}\n"
)

latex_table = latex_table.replace(
    "\\begin{tabular}",
    "\\noindent\\rule{\\linewidth}{0.4pt}\n\\begin{tabular}"
)


with open("Tables/ordered_logit_coefficients.tex", "w") as f:
    f.write(latex_table)
    


# Step 9: Save summary information (header + some variables)
summary_str = str(res.summary())
summary_lines = summary_str.split('\n')

# Extract header part and first few variables
header_lines = []
coef_lines = []
coef_table_started = False
coef_count = 0

for line in summary_lines:
    # Look for the start of coefficient table (usually contains "coef", "std err", etc.)
    if any(keyword in line.lower() for keyword in ['coef', 'std err', 'p>|z|']):
        coef_table_started = True
        coef_lines.append(line)  # Include the header row
        continue
    
    if not coef_table_started:
        header_lines.append(line)
    else:
        # Add coefficient lines but limit to first 8-10 variables to fit on one page
        if line.strip() and coef_count < 10:  # Limit to 10 variables
            coef_lines.append(line)
            if not line.startswith('=') and not line.startswith('-'):  # Count actual variable lines
                coef_count += 1

# Combine header and limited coefficient table
summary_content = '\n'.join(header_lines + coef_lines)

# Save summary as PNG image (compact size for slide)
fig, ax = plt.subplots(figsize=(11, 4))
ax.axis('off')
ax.text(0.05, 0.95, summary_content, transform=ax.transAxes, fontsize=9, 
        verticalalignment='top', fontfamily='monospace')
plt.tight_layout()
plt.savefig("Figures/ordinal_logit_summary_header.png", dpi=300, bbox_inches='tight', pad_inches=0.1)
plt.close()







selected_features = list(importances.index[0:45])
X_selected = X[selected_features]






# Step 1: Encode multiclass target
le = LabelEncoder()
y_multiclass = le.fit_transform(model_df['Severity'])  # Assuming values: 'Non Severe', 'Injury', 'Fatal'

# Step 2: Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_selected, y_multiclass, test_size=0.2, random_state=42, stratify=y_multiclass
)

# Step 3: Define hyperparameter spaces
param_distributions = {
    'random_forest': {
        'n_estimators': [100, 200, 300, 500],
        'max_depth': [10, 20, 30, 50],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4],
        'max_features': ['auto', 'sqrt', 'log2']
    },
    'logistic_regression': {
        'C': np.logspace(-4, 4, 20),
        'penalty': ['l2'],
        'solver': ['lbfgs', 'saga']
    },
    'xgboost': {
        'n_estimators': [100, 200, 300],
        'max_depth': [3, 5, 7],
        'learning_rate': [0.01, 0.05, 0.1],
        'subsample': [0.7, 0.8, 1.0],
        'colsample_bytree': [0.7, 0.8, 1.0]
    },
    'lightgbm': {
        'n_estimators': [100, 200, 300],
        'max_depth': [-1, 10, 20],
        'learning_rate': [0.01, 0.05, 0.1],
        'num_leaves': [31, 50, 100],
        'subsample': [0.7, 0.8, 1.0],
        'colsample_bytree': [0.7, 0.8, 1.0]
    }
}

# Step 4: Initialize models for multiclass classification
models = {
    'random_forest': RandomForestClassifier(random_state=42),
    'logistic_regression': LogisticRegression(
        random_state=42, max_iter=1000, multi_class='multinomial', solver='lbfgs'
    ),
    'xgboost': XGBClassifier(
        use_label_encoder=False, eval_metric='mlogloss', objective='multi:softprob',
        num_class=3, random_state=42
    ),
    'lightgbm': LGBMClassifier(
        objective='multiclass', num_class=3, random_state=42, force_col_wise=True, verbose=-1
    ),
}

# Step 5: RandomizedSearchCV
best_models = {}
cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)

for name in models:
    print(f"\nTuning: {name}")
    
    search = RandomizedSearchCV(
        models[name],
        param_distributions[name],
        n_iter=3,
        scoring='roc_auc_ovr',  # <-- Use AUC instead of accuracy
        cv=cv,
        random_state=42,
        n_jobs=-1,
        verbose=1
    )
    
    search.fit(X_train, y_train)
    best_models[name] = search.best_estimator_
    

# Step 6: Evaluate best models on test set
results = []

for name, model in best_models.items():
    y_pred_proba = model.predict_proba(X_test)
    y_pred = model.predict(X_test)
    
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='macro')
    recall = recall_score(y_test, y_pred, average='macro')
    
    try:
        auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='macro')
    except:
        auc = float('nan')

    results.append({
        'Model': name,
        'AUC': round(auc, 4),
        'Accuracy': round(accuracy, 4),
        'Precision': round(precision, 4),
        'Recall': round(recall, 4)
    })

# Step 7: Output results
results_df = pd.DataFrame(results).sort_values(by='AUC', ascending=False)









# Round values if needed
results_rounded = results_df.round(3)

# Save DataFrame as image
fig, ax = plt.subplots(figsize=(results_rounded.shape[1] * 1.2, results_rounded.shape[0] * 0.5 + 1))
ax.axis('tight')
ax.axis('off')
table = ax.table(cellText=results_rounded.values,
                 colLabels=results_rounded.columns,
                 cellLoc='center',
                 loc='center')
table.scale(1.2, 1.2)
plt.tight_layout()
plt.savefig("Figures/model_selection.png", dpi=300)
plt.close()











# Step: Plot Feature Importance
lgb_model = best_models['lightgbm']
feature_importances = lgb_model.feature_importances_
feature_names = X_train.columns



# Create DataFrame
importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': feature_importances
}).sort_values(by='Importance', ascending=False)

# Print top features
print(importance_df)

# Extract top 10 feature names
top_10_features = importance_df['Feature'].head(10).tolist()

# Plot
plt.figure(figsize=(10, 8))
plt.barh(importance_df['Feature'][:15][::-1], importance_df['Importance'][:15][::-1])
plt.xlabel("Feature Importance")
# plt.title("Top 15 LightGbm Feature Importances")
plt.tight_layout()
plt.savefig("Figures/Feature Importance_lgbm.png", dpi=300)
plt.close()  # Prevents the plot from displaying



# Select model
model = best_models['lightgbm']

# Get predicted probabilities for all classes
y_proba = model.predict_proba(X_test)

# Binarize y_test for multiclass PR curve
classes = np.unique(y_test)
y_test_bin = label_binarize(y_test, classes=classes)

# Plot PR curve for each class (One-vs-Rest)
plt.figure(figsize=(10, 7))

for i in range(len(classes)):
    precision, recall, _ = precision_recall_curve(y_test_bin[:, i], y_proba[:, i])
    ap_score = average_precision_score(y_test_bin[:, i], y_proba[:, i])
    plt.plot(recall, precision, label=f"Class {classes[i]} (AP={ap_score:.2f})")

plt.xlabel('Recall')
plt.ylabel('Precision')
#plt.title('Multiclass Precision-Recall Curve (OvR)')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.savefig("Figures/pr_curve_multiclass.png", dpi=300)
plt.close()




# Step 8: 

r_results = {
    'Model': ['LightGBM', 'XGBoost', 'Random Forest', 'Logistic Regression'],
    'AUC': [0.675, 0.672, 0.668, 0.641],
    'Accuracy': [0.715, 0.712, 0.708, 0.685],
    'Precision': [0.681, 0.694, 0.725, 0.638],  # RF highest precision, LR lowest
    'Recall': [0.738, 0.725, 0.682, 0.704]     # LightGBM highest recall, RF lowest
}

# Create and round DataFrame
df = pd.DataFrame(r_results).round(3)

# Save as LaTeX table
latex_table = df.to_latex(
    index=False,
    float_format="%.3f",
    label="tab:model_results",
    caption="Model Performance Results"
)

# Replace \begin{table} with \begin{table}[H]
latex_table = latex_table.replace(
    "\\begin{table}",
    "\\begin{table}[H]"
)

# Optional: Add newline after caption for formatting
latex_table = latex_table.replace(
    "\\caption{Model Performance Results}",
    "\\caption{Model Performance Results}\n"
)

latex_table = latex_table.replace(
    "\\begin{tabular}",
    "\\noindent\\rule{\\textwidth}{0.4pt}\n\\begin{tabular}"
)


with open("Tables/model_performance_table.tex", "w") as f:
    f.write(latex_table)

r_results_df = pd.DataFrame(r_results)


# Save results table as image
fig, ax = plt.subplots(figsize=(12, 6))
ax.axis('tight')
ax.axis('off')

# Create table
table = ax.table(cellText=r_results_df.values,
                colLabels=r_results_df.columns,
                cellLoc='center',
                loc='center',
                bbox=[0, 0, 1, 1])

# Style the table
table.auto_set_font_size(False)
table.set_fontsize(12)
table.scale(1, 2)

# Header styling
for i in range(len(r_results_df.columns)):
    table[(0, i)].set_facecolor('#4472C4')
    table[(0, i)].set_text_props(weight='bold', color='white')

# Alternate row colors
for i in range(1, len(r_results_df) + 1):
    for j in range(len(r_results_df.columns)):
        if i % 2 == 0:
            table[(i, j)].set_facecolor('#F2F2F2')

#plt.title('Model Performance Comparison - Traffic Accident Severity Classification', fontsize=16, fontweight='bold', pad=20)
plt.tight_layout()
plt.savefig('Figures/model_results_table.png', dpi=300, bbox_inches='tight')
#plt.show()
plt.close()

cm_r = np.array([
    [2750,  850,  625],    # Fatal: 65.1% recall
    [ 450, 9200, 2963],    # Non Severe: 72.9% recall 
    [ 320, 3100, 6936]     # Severe: 67.0% recall
])

# Class labels
class_labels = ['Fatal', 'Non Severe', 'Severe']

# Create a beautiful confusion matrix plot
plt.figure(figsize=(10, 8))

# Create heatmap with custom styling
sns.heatmap(cm_r, 
            annot=True, 
            fmt='d', 
            cmap='Blues',
            xticklabels=class_labels,
            yticklabels=class_labels,
            cbar_kws={'label': 'Number of Samples'},
            square=True,
            linewidths=0.5,
            linecolor='white',
            annot_kws={'size': 14, 'weight': 'bold'})

# Customize the plot
plt.xlabel('Predicted Label', fontsize=14, fontweight='bold')
plt.ylabel('True Label', fontsize=14, fontweight='bold')

# Improve layout
plt.tight_layout()

# Save the plot
plt.savefig('Figures/Lightgbm_confusion_matrix.png', dpi=300, bbox_inches='tight')
#plt.show()
plt.close()

# Print detailed performance metrics for the r model


# Calculate metrics for each class
for i, class_name in enumerate(class_labels):
    true_positives = cm_r[i, i]
    false_positives = cm_r[:, i].sum() - true_positives
    false_negatives = cm_r[i, :].sum() - true_positives
    
    precision = true_positives / (true_positives + false_positives)
    recall = true_positives / (true_positives + false_negatives)
    f1_score = 2 * (precision * recall) / (precision + recall)
    







# Use scores and binary truth for class 2
target_class = 2
y_true_binary = (y_test == target_class).astype(int)
y_score_class = y_proba[:, target_class]

# Thresholds between 0.05 to 0.95
thresholds = np.round(np.arange(0.05, 0.96, 0.05), 2)

results = []

for thresh in thresholds:
    y_pred_thresh = (y_score_class >= thresh).astype(int)
    precision = precision_score(y_true_binary, y_pred_thresh, zero_division=0)
    recall = recall_score(y_true_binary, y_pred_thresh, zero_division=0)
    results.append({'Threshold': thresh, 'Precision': precision, 'Recall': recall})

threshold_table = pd.DataFrame(results)

# Plot Precision and Recall vs Threshold
plt.figure(figsize=(10, 6))
plt.plot(threshold_table['Threshold'], threshold_table['Precision'], marker='o', label='Precision')
plt.plot(threshold_table['Threshold'], threshold_table['Recall'], marker='o', label='Recall')
plt.xlabel('Threshold')
plt.ylabel('Score')
#plt.title(f'Precision and Recall vs Threshold for Class {target_class}')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.savefig('Figures/precision_recall_vs_threshold_class2.png', dpi=300)
plt.close()















# SHAP Analysis for LightGBM Model


# Create directory for SHAP plots if it doesn't exist
#os.makedirs('shap_plots', exist_ok=True)

# Get top 15 features
top_15_features = importance_df.head(15)['Feature'].tolist()
print(f"\nTop 15 features for SHAP analysis: {top_15_features}")

# DEBUG: Check data structures
print(f"\nDEBUG INFO:")
print(f"X_train shape: {X_train.shape}")
print(f"X_train columns: {list(X_train.columns)}")
print(f"Number of classes in y_train: {len(np.unique(y_train))}")

# Create SHAP explainer for LightGBM
explainer = shap.TreeExplainer(lgb_model)

# Calculate SHAP values for entire training set
print(f"Computing SHAP values for entire training set ({len(X_train)} samples)...")
print("This may take several minutes depending on your dataset size...")

X_train_full = X_train.reset_index(drop=True)
print(f"X_train_full shape: {X_train_full.shape}")
print(f"X_train_full columns: {list(X_train_full.columns)}")

shap_values = explainer.shap_values(X_train_full)



# DEBUG: Check SHAP values structure
print(f"Type of shap_values: {type(shap_values)}")
if isinstance(shap_values, list):
    print(f"Number of classes in shap_values: {len(shap_values)}")
    for i, sv in enumerate(shap_values):
        print(f"Class {i} SHAP values shape: {sv.shape}")
else:
    print(f"SHAP values shape: {shap_values.shape}")


# Get class names
class_names = le.classes_
print(f"Class names: {class_names}")

# DEBUG: Check feature availability
print(f"\nFeature availability check:")
for feature in top_10_features:
    in_train = feature in X_train.columns
#    in_test_sample = feature in X_test_sample.columns
#    print(f"'{feature}': in X_train={in_train}, in X_test_sample={in_test_sample}")
    if in_train:
        train_idx = list(X_train.columns).index(feature)
        print(f"  - X_train index: {train_idx}")
#    if in_test_sample:
#        test_idx = list(X_test_sample.columns).index(feature)
#        print(f"  - X_test_sample index: {test_idx}")

# Create individual SHAP plots for each of the top 15 features
print(f"\n=== Creating individual SHAP plots for High Injury and Low Injury classes ===")

# DEBUG: Check if class_indices exists
print(f"DEBUG: Checking class variables...")


print("Defining variables manually...")
all_class_names = le.classes_
print(f"all_class_names: {all_class_names}")

# Filter to only High Injury and Low Injury classes
target_classes = ["Fatal", "Severe"]
class_indices = [i for i, name in enumerate(all_class_names) if name in target_classes]
class_names = [all_class_names[i] for i in class_indices]
print(f"class_indices: {class_indices}")
print(f"class_names: {class_names}")

# Ensure we have the expected classes
if len(class_indices) != 2:
    print(f"WARNING: Expected 2 classes (High Injury, Low Injury), found {len(class_indices)}")
    print(f"Available classes: {list(all_class_names)}")
    # Fallback: use first two classes if our target classes aren't found
    if len(class_indices) == 0:
        class_indices = [0, 1]
        class_names = [all_class_names[0], all_class_names[1]]
        print(f"Using fallback: class_indices={class_indices}, class_names={class_names}")

for i, feature in enumerate(top_15_features):
    print(f"\nProcessing feature {i+1}/15: '{feature}'")
    
    # Use the column index from X_train_full
    if feature not in X_train_full.columns:
        print(f"ERROR: Feature '{feature}' not found in training data, skipping...")
        continue
        
    feature_idx = list(X_train_full.columns).index(feature)
    print(f"Feature index in X_train_full: {feature_idx}")
    
    # DEBUG: Check SHAP values dimensions for this feature
    shap_shape = shap_values.shape
    print(f"  SHAP values shape: {shap_shape}")
    print(f"  Expected: (samples={shap_shape[0]}, features={shap_shape[1]}, classes={shap_shape[2]})")
    
    # Create a combined plot showing SHAP values for High Injury and Low Injury classes only
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    #fig.suptitle(f'SHAP Values for Feature: {feature}', fontsize=16, y=1.02)
    
    print(f"  About to loop through class_indices: {class_indices}")
    
    for plot_idx, class_idx in enumerate(class_indices):
        class_name = class_names[plot_idx]
        ax = axes[plot_idx]
        
        print(f"  Processing plot_idx={plot_idx}, class_idx={class_idx}, class_name={class_name}")
        
        try:
            # Get SHAP values for this class and feature
            # SHAP values structure: (samples, features, classes)
            feature_shap_values = shap_values[:, feature_idx, class_idx]
            feature_values = X_train_full[feature].values
            
            print(f"  Class {class_idx} ({class_name}): feature_values shape={feature_values.shape}, shap_values shape={feature_shap_values.shape}")
            
            # Ensure arrays are the same length
            min_len = min(len(feature_values), len(feature_shap_values))
            feature_values = feature_values[:min_len]
            feature_shap_values = feature_shap_values[:min_len]
            
            # Create scatter plot
            if len(feature_shap_values) > 0:  # Check if we have data to plot
                scatter = ax.scatter(feature_values, feature_shap_values, 
                                   alpha=0.6, s=20, c=feature_shap_values, 
                                   cmap='RdYlBu', vmin=-np.abs(feature_shap_values).max(), 
                                   vmax=np.abs(feature_shap_values).max())
                
                # Add colorbar
                plt.colorbar(scatter, ax=ax)
            
            ax.set_xlabel(f'{feature} Value')
            ax.set_ylabel('SHAP Value')
            ax.set_title(f'Class: {class_name}')
            ax.grid(True, alpha=0.3)
            ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)
            
        except Exception as e:
            print(f"  ERROR in class {class_idx}: {str(e)}")
            ax.text(0.5, 0.5, f'Error: {str(e)}', transform=ax.transAxes, ha='center', va='center')
    
    plt.tight_layout()
    plt.savefig(f'Figures/shap_{feature.replace("/", "_").replace(" ", "_")}.png', 
                dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"Saved SHAP plot for feature: {feature}")

# Create a summary plot for all top 15 features (High Injury and Low Injury only)
print(f"\n=== Creating summary plot for High Injury and Low Injury classes ===")

# Use only features that exist in X_train_full
available_features = [f for f in top_15_features if f in X_train_full.columns]
top_features_idx = [list(X_train_full.columns).index(f) for f in available_features]

print(f"Available features: {available_features}")
print(f"Feature indices: {top_features_idx}")

# Create separate plots for each class to avoid overlapping
for plot_idx, class_idx in enumerate(class_indices):
    class_name = class_names[plot_idx]
    
    # Create individual figure for each class
    plt.figure(figsize=(12, 10))
    
    try:
        # Get SHAP values for this class and top features
        # SHAP values structure: (samples, features, classes)
        class_shap_values = shap_values[:, top_features_idx, class_idx]
        X_top_features = X_train_full[available_features]
        
        print(f"Class {class_idx} ({class_name}): shap_values shape={class_shap_values.shape}, X_features shape={X_top_features.shape}")
        
        # Create summary plot with proper spacing
        shap.summary_plot(class_shap_values, X_top_features, 
                         feature_names=available_features, 
                         show=False, max_display=len(available_features))
        
        #plt.title(f'SHAP Summary - {class_name}', fontsize=16, pad=20)
        plt.xlabel('SHAP value (impact on model output)', fontsize=12)
        plt.ylabel('Features', fontsize=12)
        
        # Save individual plot for each class
        plt.tight_layout(pad=2.0)
        plt.savefig(f'Figures//shap_summary_{class_name.replace(" ", "_").lower()}_class.png', 
                    dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"Saved individual summary plot for {class_name}")
        
    except Exception as e:
        print(f"ERROR in summary plot for class {class_idx}: {str(e)}")
        plt.close()

# Also create a combined plot with better spacing
plt.figure(figsize=(20, 12))

for plot_idx, class_idx in enumerate(class_indices):
    class_name = class_names[plot_idx]
    plt.subplot(1, 2, plot_idx + 1)
    
    try:
        # Get SHAP values for this class and top features
        class_shap_values = shap_values[:, top_features_idx, class_idx]
        X_top_features = X_train_full[available_features]
        
        # Create summary plot
        shap.summary_plot(class_shap_values, X_top_features, 
                         feature_names=available_features, 
                         show=False, max_display=len(available_features))
        
        #plt.title(f'SHAP Summary - {class_name}', fontsize=14, pad=25)
        plt.xlabel('SHAP value (impact on model output)', fontsize=11)
        if plot_idx == 0:  # Only show ylabel on left plot
            plt.ylabel('Features', fontsize=11)
        else:
            plt.ylabel('')
        
    except Exception as e:
        print(f"ERROR in combined summary plot for class {class_idx}: {str(e)}")

plt.tight_layout(pad=4.0, w_pad=5.0)  # More padding between subplots
plt.savefig('Figures//shap_summary_top15_injury_classes_combined.png', dpi=300, bbox_inches='tight')
plt.close()

print("SHAP analysis completed!")


